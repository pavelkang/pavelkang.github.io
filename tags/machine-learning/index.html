<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Tag: machine learning | Kai Kang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="a place of sharing tech, food">
<meta property="og:type" content="website">
<meta property="og:title" content="Kai Kang">
<meta property="og:url" content="https://pavelkang.github.io/tags/machine-learning/">
<meta property="og:site_name" content="Kai Kang">
<meta property="og:description" content="a place of sharing tech, food">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kai Kang">
<meta name="twitter:description" content="a place of sharing tech, food">

  
    <link rel="alternative" href="/atom.xml" title="Kai Kang" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">

  
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-53656983-2', 'auto');
ga('send', 'pageview');

</script>


</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Kai Kang</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Carpe diem</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/resume">Resume</a>
        
          <a class="main-nav-link" href="http://www.kaikang.me">Visit My Home Page</a>
        
      </nav>
      <nav id="sub-nav">
        <a id="nav-github-link" class="nav-icon" href="https://github.com/pavelkang"></a>
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><input type="submit" value="&#xF002;" class="search-form-submit"><input type="hidden" name="q" value="site:https://pavelkang.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Machine-Learning-Notes" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/01/21/Machine-Learning-Notes/" class="article-date">
  <time datetime="2015-01-21T15:41:09.000Z" itemprop="datePublished">Jan 21 2015</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/01/21/Machine-Learning-Notes/">Machine Learning Notes</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody" style="display:none">
      
        <h3 id="Undirected_Graphical_Models">Undirected Graphical Models</h3>
<ul>
<li>Inference, parameter learning (MLE), Structure learning (Prior over graphs, Regularization)</li>
<li>Markov Blanket</li>
<li>DGM but not UGM: rain, sprinkle -&gt; wet</li>
<li>Inference: Generally intractable (approx. algorithms: MCMC)</li>
<li>Parameter Learning:<ul>
<li>Complete Data -&gt; Iterative Methods, max ent: doable but non-trivial</li>
<li>Incomplete Data -&gt; E-M</li>
<li>Cannot generate data for undirected graphs (very hard)</li>
<li>How to generate data from Undirected Graphical Model<ul>
<li>Random Walk</li>
</ul>
</li>
<li>Supervised learning: Data are labeled</li>
<li>Query: Nearest neighbor (works for classification and regression)</li>
</ul>
</li>
</ul>
<h3 id="Bayesian_Belief_Network">Bayesian Belief Network</h3>
<ul>
<li>every node, conditioned on all immediate parents, is independent of all non-descendants</li>
<li>$O(2^{|\text{Parents}|})$: Factorization.</li>
<li>c -&gt; r -&gt; w. c is not independent of w. However, conditioned on KNOWing r, w is independent of c.</li>
<li>Parameter Learning in DGM</li>
<li>If there are complete data, then just need to worry about smoothing</li>
<li>incomplete -&gt; EM algorithm. MLE from incomplete data</li>
<li>Structure learning</li>
</ul>
<ol>
<li>initialize u1, u2, … to some guess (centers of clusters of data points)</li>
<li>calculate “probabilistic assignment” of each $x_i$:</li>
<li>Re-estimate</li>
</ol>
<p>1st-order Markov Assumption</p>
<p>L-Z compression(1/3)<br>本来有一段信息basketball，compress。这需要compression algorithm to be adaptive<br>做20次添加到basketball的末尾，比对compress的结果。最后compression最短的那个添加的东西和basketball最有关</p>
<p>传送信息的时候，有小的DT但是有exceptions, 我们要找到rule + 处理exception的平衡（最短的）<br>MDL: Minimum Description Length</p>
<h3 id="Naive_Bayes_Algorithm">Naive Bayes Algorithm</h3>
<ul>
<li>Conjunction of attributes to a finite set of output</li>
</ul>
<h1 id="Lecture_19_Naive_Bayes">Lecture 19 Naive Bayes</h1>
<ul>
<li>Conditional independent vs. truly independent<ul>
<li>z = x XOR y: if z=0, then x tells me y, y tells me x, but x, y independent</li>
</ul>
</li>
<li>Naive Bayes: overly confident<ul>
<li>因为对于不是iid的data，假设了他们都independent，都乘起来就会overly confident</li>
</ul>
</li>
</ul>
<h1 id="Lecture_17">Lecture 17</h1>
<ul>
<li>Find $h_map$:<ul>
<li>如果不好直接算最大值就取log。和对likelihood取log，算导数一样</li>
</ul>
</li>
<li>Example: Gaussian<ul>
<li>Given data $x_1, x_2, …$, assume $\sigma^2 = 1$, estimate u</li>
<li>hard bias: choice of gaussian; soft: max-likelihood method<br>-</li>
</ul>
</li>
</ul>
<h1 id="Lecture_16">Lecture 16</h1>
<ul>
<li>Q1: How good is a hypothesis?<ul>
<li>True error: $Err_D(h)$ (confidence interval of true error)</li>
</ul>
</li>
<li>Q2: Is my hypothesis better than yours:<ul>
<li>$d = Err_Dh_B - Err_Dh_A$</li>
<li>$\hat{d} = Err<em>{SA}….Err</em>{SB}$: unbiased estimation (notes有公式，很像confidence interval. SA SB are two samples better be similar. Pair Test. 比如一个班做两套不同的卷子是不合理的)</li>
</ul>
</li>
<li>Q3: Is my learning algorithm better than yours:<ul>
<li>$h=L_A(s)$ (s is the training sample)</li>
<li>We compare $Err_D(L_A(s))$, $Err_D(L_B(s))$. (s是蓝色的(observed), 所以这两个数都是observed，所以对于不同的training sample, 结果可能不一样。所以变成左右两边取expetation.)</li>
<li>对于一些s当做training，用test sample测error，可以找到d hat。如果sample有限就每次拿出来一点当training另外的当testing</li>
</ul>
</li>
</ul>
<h3 id="Bayesian_Learning">Bayesian Learning</h3>
<ul>
<li>Probabilistic soft bias. $\forall h\in H, p(h)$ captures initial belief.</li>
<li>Consistency -&gt; Match $P(D|h)$: likelihood</li>
<li>Bayes Game<ul>
<li>Start with $H = (h_1, h_2, …, h_n)$. PRIOR(INITIAL): $p(h)$.</li>
<li>Data D</li>
<li>Likelihood $P(D|h)$</li>
<li>Bayes formula(MUST)<ul>
<li>$P(h, D) = P(h) <em> P(D|h) = P(D) </em> P(h|D)$</li>
<li>$P(h)$ captures the belief BEFORE seeing the data, PRIOR</li>
<li>$P(h|D)$ captures … AFTER, POSTERIOR</li>
<li>$P(h|D) = \frac{P(D|h) * P(h)}{P(D)}$</li>
<li>If we have some prior belief and likelihood, after we see some data, the posterior is something we should believe. Bayesian learning updates your belief about life.</li>
<li>$h_{map}$是maximize posterior $p(h|D)$的h</li>
<li>如果$p(h)=\frac{1}{|H|}$, 那就相当于maximize$p(D|h)$, likelihood. 也叫 max likelihood hypothesis</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="Lecture_1">Lecture 1</h1>
<h3 id="Jan_14_Wed">Jan 14 Wed</h3>
<hr>
<p><strong>Machine Learning</strong>: program that gets better at some tasks(functions) with experiences(data, examples)<br>Formalizing a task as a function learning task (f: board pos -&gt; next move)<br><em>Human Annotation</em>: human assigning scores</p>
<ul>
<li>f: INPUT -&gt; {choice1, choice2, …} : <em>classification</em></li>
<li>f: INPUT -&gt; Real : <em>Regression</em></li>
<li>f: INPUT -&gt; {0, 1} probability: <em>Logistic Regression</em></li>
<li>f: INPUT -&gt; P(INPUT) <em>Probability Density Estimation</em><br>Binary Classification = Concept Learning = Learning a subset of $X$.</li>
</ul>
<h1 id="Lecture_2">Lecture 2</h1>
<h3 id="Wed_Jan_21_10:48:53_EST_2015">Wed Jan 21 10:48:53 EST 2015</h3>
<hr>
<ul>
<li><em>Input space</em>: A set of features</li>
<li><em>Target Concept</em>: The concept or function to be learned, denoted by $c$, a boolean-<br>valued function defined over the instances $X$; that is, $c$ : $X \rightarrow {0, 1}$</li>
<li><em>Training Examples</em>: $D$</li>
<li><em>Hypothesis</em> Conjunctions of literals. Eg $&lt;?, Cold, High, ?, ?&gt;$. We are looking for $h$ in the set of all possible hypothesis $H$ such that $\forall x, h(x) = c(x)$</li>
<li>$|X| \ll |H| \ll |C| = 2^x$, $|C|$ is the set of all possible functions (concept space)</li>
<li>X is input space, H is hypothesis space (each item + 1), C is concept space ($2^{|X|}$)</li>
</ul>
<h1 id="Lecture_3">Lecture 3</h1>
<p>$D={(x_1, c(x_1)), …}$ data<br>Each concept c_i is a bit vector.</p>
<p><strong>Hypothesis consistency</strong> $Consistent(h, D) = \forall <x, c(x)="">\in D, h(x) = c(x)$<br><strong>Version space</strong> $VS_{H,D} = {h\in H| Consistent(h, D)}$</x,></p>
<h3 id="Find-S_algorithm">Find-S algorithm</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">hypo = &lt;null, null, ...&gt; <span class="comment"># says NO to everything</span></div><div class="line"><span class="comment"># D is the data {(x1, c(x1)), (x2, c(x2)), ...}</span></div><div class="line"><span class="comment"># looking for target concept c in C</span></div><div class="line"><span class="keyword">for</span> positive_example <span class="keyword">in</span> D:</div><div class="line">    <span class="keyword">for</span> attribute_constraint <span class="keyword">in</span> hypo:</div><div class="line">        <span class="keyword">if</span> pos_example satisfied by attr_constraint: <span class="keyword">pass</span></div><div class="line">        <span class="keyword">else</span>: hypo := next more general constraint satisfied by attr_constraint</div><div class="line"><span class="keyword">return</span> hypo</div></pre></td></tr></table></figure>

<h3 id="Problems_with_Find-S">Problems with Find-S</h3>
<ul>
<li>Fails when training data inconsistent</li>
<li>Picks a maximally specific $h$</li>
</ul>
<h3 id="List-Then-Eliminate">List-Then-Eliminate</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hypos = a list containing every hypothesis</div><div class="line"><span class="keyword">for</span> &lt;x, c(x)&gt; <span class="keyword">in</span> D:</div><div class="line">    remove h <span class="keyword">in</span> hypos <span class="keyword">if</span> h(x) != c(x)</div><div class="line"><span class="keyword">return</span> hypos</div></pre></td></tr></table></figure>

<h1 id="Lecture_4_Informaiton_theory">Lecture 4 Informaiton theory</h1>
<p><strong>Bias</strong></p>
<ul>
<li>Hard bias: eg. “Language bias” (conjunctions)</li>
<li>Soft bias: “ranking”, preference bias (prefer hypothesis that tends to say NO)</li>
</ul>
<p>Information = Reduction in Uncertainty<br>Suprisal = information received when observing an outcome = $I(\text{sunny}) = log_2 \frac{1}{prob(\text{sunny})} \in [0, \infty)$<br>Information is <em>additive</em>:</p>
<h1 id="Lecture_5">Lecture 5</h1>
<p>Suprise(Information learned) = $log<em>2\frac{1}{p(e)}$, where $p(e)$ is <em>subjective</em>, int <strong>BITS</strong>.<br><strong>Cross Entropy</strong> $\Sigma p_i I(s_i)$.<br><strong>Fundamental Inequality</strong><br><strong>Distance</strong> $D</em>{KL}(p||q) = E_p[log\frac{p(x)}{q(x)}]\ge 0$<br>A special case would be $p = q$, when we fully know the “truth”. $0\lte CH(p, p) \lte log k$, where $log k$ is fully uniform. <em>20 questions game</em>.<br>Ask questions which answers you think are equally likely to be $0$ and $1$, so that you can learn the most information.</p>
<p>$H(Letter) = log{27} = 4.75 BITS$.</p>
<h1 id="Lecture_6">Lecture 6</h1>
<p>Entropy is a tight lower bound of efficiency of emitting events.<br><em>Regular Variable</em>: holds a <em>single</em> value.<br><em>Random Variable</em>: distribution<br>Joint Entropy, Conditional Entropy, Average Conditional Entropy.<br>Average Conditional Entropy, $H(T/M)$.<br>$H(T) - H(M/T)$, how much $M$ is telling us on average about $T$.<br>$H(T) - H(M/T) = H(M) - H(M/T)$<br>$I(X; Y; Z) = I(X,Y) - I(X,Y|Z)$<br>$I(X, Y) = H(X) - H(X|Y)$<br>$I(y, {x_1, x_2, …}) = I(y, x_1) + (y; x_2|x_1) + I(y, x_3|x_1, x_2) + …$</p>
<h1 id="Lecture_7">Lecture 7</h1>
<h3 id="Decision_Tree">Decision Tree</h3>
<p>Same attribute will not appear twice in a single path</p>
<h1 id="Lecture_8">Lecture 8</h1>
<p>Draw a decision tree from $f(A, B, C, D, E) = (A\wedge B)\vee (C\wedge \not D\wedge E)$<br>Decision trees can represent any functions.</p>
<h1 id="Lecture_9">Lecture 9</h1>
<p>Law of total variance: $VAR(y) = VAR(E[y|x]) + E[VAR(y|x)]$<br>First: how far apart the centers are<br>Second: WITHIN-CLUSTER VARIANCE</p>
<h1 id="Lecture_11">Lecture 11</h1>
<h3 id="Linear_Regression">Linear Regression</h3>
<p>To a multi-linear regression: $\beta<em>{OLS} = (X^TX)^{-1}(X^TY)$<br>Hard bias: …<br>Soft bias: square of residue<br>_Sparse Estimation</em>: p &gt;&gt; n. $\beta$ is non-identifiable</p>
<ul>
<li>L2 Norm(sum of squared beta): “Ridge Regression”</li>
<li>L1 Norm(sum of abs beta): “Lasso Regression”</li>
<li>L0 Norm(number of non-zero beta)<br><em>Regularization</em>: minimize $argmin(Error(data, param) + Complexity(Param))$<br>In this example: square of residues + norm of Beta</li>
</ul>
<h1 id="Lecture_12">Lecture 12</h1>
<h3 id="Neural_Networks">Neural Networks</h3>
<p>Works really well with <em>complex real-world sensor-data</em><br>Since human neurons switch slowly compared to computers, but is able to do complicated computations quickly. It is conjectured that humans do parallel computation.<br>Learning corresponds to assigning weights to edges in an acyclic directed graph.</p>
<p><em>Perceptrons</em>: Takes a vector of real inputs, calculates a linear comb. If bigger threshold, output 1; 0 otherwise<br>Learning a perceptron = choosing values for the <em>coeffs</em>!<br>It can represent primitive boolean functions AND, OR, NAND, …<br>And all boolean functions in general<br>Training a perceptron: start with arbitrary weights, repeat… (p88)<br>to modify $w_i$ to new $w_i$: $\Delta w_i = \ita (t-o)x_i$, where $\ita$ is the Learning Rate, which is a small value.</p>
<h1 id="Lecture_13">Lecture 13</h1>
<p>Single Linear Unit / Network of L.U.’s / Perceptron(STEP fxn,Classifier) / Network of Perceptrons<br>Expressive Power(hard bias): All Linear fxns / Still Linear, / Linear Classifiers / Any decision problem<br>Optimization Criterion(soft bias): Mean Squared Error / Same / Misclassific Rate / Same<br>Computational complexity: Algebraically, Gradient Decent / Same / Perceptron Learning Rule / Not continuous (No known feasible algo.)</p>
<p>(no bias -&gt; no learning)<br>Sigmoid function:<br>Network of Sigmoid: any function / MSE / Gradient Decent (exists local minimal)</p>
<p><em>Back Propagation</em>:</p>
<h1 id="Midterm_Prep">Midterm Prep</h1>
<p>Data mining: extract information from a large data set and transform it to a structure for future use<br>Target function<br>Hypothesis: h is a <em>conjunction</em> of constraints on attributes<br>constraints: value; don’t care; no value allowed;<br>Concept learning:<br>Training example: &lt;$x_1$, c($x_1$)&gt;, where c is the <em>target function</em> or <em>concept</em>.<br>Determine: h such that h(x) = c(x) for all x in D<br>Find-S: h0 = most specific (no to everything); for each pos example, “merge” the attribute constraint<br>consistent: (h, D) if h(x) = c(x) for all c in D<br>version space: subset of H(hypothesis space) consistent with all training data D<br>List-then-Eliminate: start with the biggest hypothesis space; find version space by checking each training example<br>Input Space: X, Hypothesis Space: H (+1), Concept Space: $2^{|X|}$</p>
<p>Information Theory<br>Information: Reduction in uncertainty: $I(E)=log_2\frac{1}{P(E)}$<br>Entropy: From a Zero-memory source S, the entropy is $\Sigma_i p_i * I(s_i) = E[log\frac{1}{p_i}]$<br>avg amount of information<br>H(T, M) &lt; H(T) + H(M) ???<br>q distribution ???<br>H(T) - H(T|M) = H(M) - H(M|T) ???</p>
<h1 id="Lecture_15">Lecture 15</h1>
<h3 id="Hypothesis_Testing">Hypothesis Testing</h3>
<p>Q1: Given a hypothesis, how good is it?<br>Q2: Given two hypothesis, which one is better?<br>Q3: Given two learning algorithms, which one is better?<br>Not about the portion of the mistakes in the whole input space, but the probability of making errors.(Consider when most of the examples are mistakes)<br>$Err_r(h)$ is an estimator of $Err_D(h)$.<br>$BIAS = E[EST] - TRUTH$ (Mean is around the truth)<br>$VARIANCE = E[EST - E[EST]]^2$(Also needs to be accurate (sharp around the truth))<br>True Error to Sample Error (Probability calculation)<br>Sample Error to True Error (Statistical Inferrence)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://pavelkang.github.io/2015/01/21/Machine-Learning-Notes/" data-id="vcrxzwcsv358nhfc" class="article-share-link">Share</a>
      
        <a href="https://pavelkang.github.io/2015/01/21/Machine-Learning-Notes/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/notes/">notes</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/251/">251</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Interview/">Interview</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cachelab/">cachelab</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/emacs/">emacs</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/food/">food</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/">machine learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/math/">math</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/notes/">notes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/schedule/">schedule</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/213/" style="font-size: 10.00px;">213</a><a href="/tags/251/" style="font-size: 20.00px;">251</a><a href="/tags/C/" style="font-size: 13.33px;">C++</a><a href="/tags/Interview/" style="font-size: 13.33px;">Interview</a><a href="/tags/Python/" style="font-size: 13.33px;">Python</a><a href="/tags/algorithm/" style="font-size: 10.00px;">algorithm</a><a href="/tags/cachelab/" style="font-size: 13.33px;">cachelab</a><a href="/tags/cachelab-213/" style="font-size: 10.00px;">cachelab 213</a><a href="/tags/cachelab-213/" style="font-size: 10.00px;">cachelab, 213</a><a href="/tags/cachelab-213/" style="font-size: 10.00px;">cachelab; 213</a><a href="/tags/combination/" style="font-size: 10.00px;">combination</a><a href="/tags/editor/" style="font-size: 10.00px;">editor</a><a href="/tags/emacs/" style="font-size: 13.33px;">emacs</a><a href="/tags/food/" style="font-size: 16.67px;">food</a><a href="/tags/hexo/" style="font-size: 13.33px;">hexo</a><a href="/tags/leetcode/" style="font-size: 10.00px;">leetcode</a><a href="/tags/lisp/" style="font-size: 10.00px;">lisp</a><a href="/tags/machine-learning/" style="font-size: 13.33px;">machine learning</a><a href="/tags/math/" style="font-size: 20.00px;">math</a><a href="/tags/notes/" style="font-size: 13.33px;">notes</a><a href="/tags/permutation/" style="font-size: 10.00px;">permutation</a><a href="/tags/schedule/" style="font-size: 13.33px;">schedule</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/10/">October 2014</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/05/10/Making-Emacs-a-Better-C-C-IDE/">Making Emacs a Better C/C++ IDE</a>
          </li>
        
          <li>
            <a href="/2015/05/06/251-Final-Review/">251 Final Review</a>
          </li>
        
          <li>
            <a href="/2015/04/06/最近想做的菜/">最近想做的菜</a>
          </li>
        
          <li>
            <a href="/2015/03/31/15251-randomness/">15251-randomness</a>
          </li>
        
          <li>
            <a href="/2015/03/23/-15-251-Probability/">[15-251] Probability</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Kai Kang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/resume" class="mobile-nav-link">Resume</a>
  
    <a href="http://www.kaikang.me" class="mobile-nav-link">Visit My Home Page</a>
  
</nav>
    
<script>
  var disqus_shortname = 'kai_kang';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">

  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>



<script src="/js/script.js" type="text/javascript"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  </div>
</body>
</html>